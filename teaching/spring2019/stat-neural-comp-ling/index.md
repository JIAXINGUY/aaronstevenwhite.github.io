---
layout: page
title: Introduction to Computational Linguistics
tagline: University of Rochester (Fall 2018)
---

# Description

This course covers advanced topics in computational linguistics, with a focus on
the deployment of statistical and neural methods for advancing linguistic theory
as well as the use of linguistic theory for designing statistical and neural
models. Topics include morphophonological grammar learning, syntactic grammar
learning, semantic representation learning for words and phrases, syntactic
parsing, semantic parsing, and natural language inference.

# Logistics

| **Classroom**    | 513 Lattimore Hall                                      |
| **Time**         | Monday & Wednesday 12:30-1:45pm                         |
|                  |                                                         |
| **Instructor**   | [Aaron Steven White](http://aswhite.net)                |
| **Office**       | 511A Lattimore Hall                                     |
| **Office hours** | Tuesday 9-11am                                          |
| **Email**        | aaron.white@rochester.edu                               |

# Readings

There will be a substantial amount of reading per week. A tentative
schedule is provided below. This tentative schedule will almost
certainly change as the semester goes forward. Consult the table below for
the most current list.

# Project

Throughout the semester you will be developing a course project that will
culminate in an ACL-style conference paper. This project consists of five main
components: (i) six checkpoints; (ii) a prospectus; (iii) a mid-semester
prospectus presentation; (iv) a final project presentation; and (v) the paper
itself. I will provide a list of potential projects at the beginning of the
semester. You may opt to do something not included in this list, but I must
approve it.

## Checkpoints

Starting in the sixth week of class, you will be required to complete weekly
checkpoints logging your progress on the project. Each checkpoint should at
least include a summary of one paper (or more) that you read for the project.
Later in the semester, as the project develops, the checkpoints should also
include a description of the code you have written for the project (along with a
link to that code on github) and/or results of running that code on some
dataset. The point of these checkpoints is two-fold: (i) to ensure that you are
making sufficient incremental progress on the project throughout the semester;
and (ii) to provide you grist for the paper, so that by the time you go to
produce a full draft, you have content to draw on.

## Prospectus

A 500-word prospectus detailing the goals of your project will be due on
Wednesday, March 20. This prospectus should specify the question your final
paper aims to answer and summarize the literature you have read so far,
including two papers you have not yet summarized in the checkpoints.

## Mid-semester presentation

You will be required to give a 15-minute presentation detailing your prospectus
to the class on either March 11 or March 13.

## Final presentation

You will be required to give a 30-minute presentation detailing your completed
project to the class on either April 29 or May 1.

## Final paper

Your final paper should use [the formatting
instructions](https://www.transacl.org/ojs/index.php/tacl/about/submissions)
specified for submission to *Transactions of the Association for Computational
Linguistics* (TACL). Students enrolled in LIN281 are required to write either
exactly four pages or exactly eight pages (though you may include appendices).
Students enrolled in LIN481 are required to write exactly ten pages (no more, no
less). These numbers are not arbitrary. A standard short paper for an ACL venue
is four pages with an optional fifth page for the "camera ready" version; a
standard long paper is eight pages with an optional ninth for the "camera ready"
version; and a standard submission to TACL is ten pages with more allowed for
revisions.

# Midterm

There will be a take-home midterm, which will tentatively be made available on
Wednesday, Oct. 24 and will be due one week later (Wednesday, Oct. 31).

# Final

For *LIN224 students*, there will be a take-home final exam, which will be made
available on Wednesday, Dec. 12 and will be due one week later (Wednesday, Dec.
19).

For *LIN424 students*, there will be a final paper, which will be due on
Wednesday, Dec. 14. A 500-word prospectus for this paper will be due Monday,
Nov. 12. More information will be made available closer to that date.

LIN224 students may opt to write a final paper in place of the final
exam, but they should do so with the knowledge that this paper will be
graded relative to a rubric designed for graduate student papers.

# Tools

I will assume competence in LaTeX and the conventional core of Python -- i.e.
built-in types, control flow, functions, classes, etc. In the first full week of
class, I will cover basic numerical computing in Python using [the scipy
stack](https://www.scipy.org/). As the class progresses, we will also use
[pytorch](https://pytorch.org/) (for neural network development) and
[pyro](http://pyro.ai/) (for probabilistic model development).

You may use any programming language that you like for developing your course
project, though I would encourage you to use Python, since it is so prevalent in
Computational Linguistics and Natural Language Processing.

All written work -- including checkpoints, the prospectus, and the final paper
*must* be produced in LaTeX. All work must furthermore be submitted
through [Overleaf](http://overleaf.com), where templates will be hosted –
i.e. I will provide templates for the checkpoints, the prospectus, and the final
paper.  Do not hand in handwritten work or email me electronic documents. You
will not receive credit for that assignment unless you subsequently turn the
assignment in through [Overleaf](http://overleaf.com) (subject to late
penalties).

# Final grades

The grading breakdown is: project checkpoints (48%), project prospectus (12%),
mid-term project presentation (10%), final project presentation (10%), project
write-up (20%). (Percentages represent percent of total grade.)

# Late work

Assignments should be submitted by 11:59pm the day they are due. An automatic
10% deduction will be applied after this time, and you must explicitly notify
both Venkat and me by email when you turn the late assignment in or you will not
receive any credit.

Starting from the end of class, assignments will lose 10% per day late according
to the UTC time-stamp of submission that Overleaf reports. Late assignments may
not be turned in for credit after a week unless explicit permission was sought
before the due date.

# Exceptions

Students will not be penalized because of important civic, ethnic,
family or religious obligations, or university service. You will be have
a chance, whenever feasible, to make up within a reasonable time any
assignment that is missed for these reasons. Absences for these reasons
will count as excused for the sake of the participation grade. But it is
your job to inform me of any expected absences in advance, as soon as
possible.

# Honesty

All assignments and activities associated with this course must be performed in
accordance with the University of Rochester's Academic Honesty Policy. More
information is available [here](http://www.rochester.edu/college/honesty/).

# Personal needs

Any student who needs special accommodations due to a disability should
let me know privately, at the start of the semester.

# Schedule

| **Date** | **Topic**                                         | **Reading**                                                                               | **Due** |
|---------|-----------------------------------------------|---------------------------------------------------------------------------------------|-----|
| Jan. 16 | Cancelled                                     | -                                                                                     |     |
| Jan. 21 | MLK Day                                       | -                                                                                     |     |
| Jan. 23 | Introduction                                  | -                                                                                     |     |
| Jan. 28 | Clustering in Python I                        | -                                                                                     |     |
| Jan. 30 | Clustering in Python II                       | -                                                                                     |     |
| Feb. 4  | Mixture Models in PythonPhonetic Categories I | -                                                                                     |     |
| Feb. 6  | Phonetic Categories I                         | Feldman et al. 2013                                                                   |     |
| Feb. 11 | Phonetic Categories II                        | -                                                                                     |     |
| Feb. 13 | Phonetic Categories IIII                      | Kronrod et al. 2016                                                                   |     |
| Feb. 18 | Phonotactic Grammars I                        | Hayes & Wilson 2008, Sec. 1-5                                                         |     |
| Feb. 20 | Phonotactic Grammars II                       | Hayes & Wilson 2008, Sec. 6-9                                                         |     |
| Feb. 25 | Phonotactic Grammars III                      | Futrell et al. 2017, Sec. 1-2                                                         |     |
| Feb. 27 | Phonotactic Grammars IV                       | Futrell et al. 2017, Sec. 3-7                                                         | C1  |
| Mar. 4  | Morphophonological Grammars I                 | Cotterell et al. 2015, Sec. 1-3                                                       |     |
| Mar. 6  | Morphophonological Grammars II                | Cotterell et al. 2015, Sec. 4-9                                                       | C2  |
| Mar. 11 | Spring Break                                  | -                                                                                     |     |
| Mar. 13 | Spring Break                                  | -                                                                                     |     |
| Mar. 18 | Project Presentations                         | -                                                                                     |     |
| Mar. 20 | Linear Models and Multilayer Perceptrons      | Goldberg 2017, Ch. 2-5                                                                | P   |
| Mar. 25 | Word Embeddings                               | Goldberg 2017, Ch. 6, 8, 10-11                                                        |     |
| Mar. 27 | Recurrent Neural Networks                     | Goldberg 2017, Ch. 15-16                                                              | C3  |
| Apr. 1  | Morphophonological Grammars III               | Goldberg 2017, Ch. 17-17.2, Faruqui et al. 2016                                       |     |
| Apr. 3  | Morphophonological Grammars IV                | -                                                                                     | C4  |
| Apr. 8  | Syntactic Parsing I                           | Linzen et al. 2016, Marvin and Linzen 2018, Gulordava et al. 2018, Wilcox et al. 2018 |     |
| Apr. 10 | Syntactic Parsing II                          | Goodkind and Bicknell 2018, van Schijndel and Linzen 2018, 2019                       | C5  |
| Apr. 15 | Natural Language Inference I                  | Bowman et al. 2015                                                                    |     |
| Apr. 17 | Natural Language Inference II                 | White et al. 2017, Poliak et al. 2018                                                 | C6  |
| Apr. 22 | Decompositional Semantic Parsing I            | Rudinger et al. 2018a, b                                                              |     |
| Apr. 24 | Decompositional Semantic Parsing II           | Zhang et al. 2018                                                                     |     |
| Apr. 29 | Project Presentations                         | -                                                                                     |     |
| May 1   | Project Presentations                         | -                                                                                     |     |

# References

* Feldman, N.H., Griffiths, T.L., Goldwater, S. and Morgan, J.L., 2013. A role for the developing lexicon in phonetic category acquisition. Psychological review, 120(4), p.751.
* Kronrod, Y., Coppess, E. and Feldman, N.H., 2016. A unified account of categorical effects in phonetic perception. Psychonomic Bulletin \& Review, 23(6), pp.1681-1712.
* Hayes, B. and Wilson, C., 2008. A maximum entropy model of phonotactics and phonotactic learning. Linguistic inquiry, 39(3), pp.379-440.
* Futrell, R., Albright, A., Graff, P. and O’Donnell, T.J., 2017. A generative model of phonotactics. Transactions of the Association for Computational Linguistics, 5, pp.73-86.
* Cotterell, R., Peng, N. and Eisner, J., 2015. Modeling word forms using latent underlying morphs and phonology. Transactions of the Association of Computational Linguistics, 3(1).
* Goldberg, Y., 2017. Neural network methods for natural language processing. Synthesis Lectures on Human Language Technologies.
* Faruqui, M., Tsvetkov, Y., Neubig, G. and Dyer, C., 2016. Morphological Inflection Generation Using Character Sequence to Sequence Learning. In Proceedings of NAACL-HLT, pp. 634-643.
* Linzen T, Dupoux E, Goldberg Y. Assessing the Ability of LSTMs to Learn Syntax-Sensitive Dependencies. Transactions of the Association for Computational Linguistics. 2016 Dec 5;4:521-35.
* Marvin, R. and Linzen, T., 2019. Targeted Syntactic Evaluation of Language Models. Proceedings of the Society for Computation in Linguistics, 2(1), pp.373-374.
* Gulordava, K., Bojanowski, P., Grave, E., Linzen, T. and Baroni, M., 2018. Colorless Green Recurrent Networks Dream Hierarchically. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers) (Vol. 1, pp. 1195-1205).
* Wilcox, E., Levy, R., Morita, T. and Futrell, R., 2018. What do RNN Language Models Learn about Filler–Gap Dependencies?. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP (pp. 211-221).
* Goodkind, A. and Bicknell, K., 2018. Predictive power of word surprisal for reading times is a linear function of language model quality. In Proceedings of the 8th Workshop on Cognitive Modeling and Computational Linguistics (CMCL 2018) (pp. 10-18).
* van Schijndel, M. and Linzen, T., 2018. Modeling garden path effects without explicit hierarchical syntax. In Proceedings of the 40th Annual Conference of the Cognitive Science Society (pp. 2600-2605).
* Bowman, S.R., Angeli, G., Potts, C. and Manning, C.D., 2015. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (pp. 632-642).
* White, A.S., Rastogi, P., Duh, K. and Van Durme, B., 2017. Inference is everything: Recasting semantic resources into a unified evaluation framework. In Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers) (Vol. 1, pp. 996-1005).
* Poliak, A., Haldar, A., Rudinger, R., Hu, J.E., Pavlick, E., White, A.S. and Van Durme, B., 2018. Collecting Diverse Natural Language Inference Problems for Sentence Representation Evaluation. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (pp. 67-81).
* Rudinger, R., White, A.S. and Van Durme, B., 2018. Neural Models of Factuality. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers) (Vol. 1, pp. 731-744).
* Rudinger, R., Teichert, A., Culkin, R., Zhang, S. and Van Durme, B., 2018. Neural Davidsonian Semantic Proto-role Labeling. arXiv preprint arXiv:1804.07976.
* Zhang, S., Duh, K. and Van Durme, B., 2018. Cross-lingual Semantic Parsing. arXiv preprint arXiv:1804.08037.
